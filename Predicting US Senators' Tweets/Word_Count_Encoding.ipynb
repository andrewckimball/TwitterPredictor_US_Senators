{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Count Encoding.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "KGUZ0DprDbG-",
        "51EmSw22DGQi",
        "J89Jo6H1DhSs",
        "jJ2JzagqfW_h"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hR2K1hFKpKR",
        "outputId": "ece0eb19-2a4d-4a1a-bee4-ef91f8b4e756"
      },
      "source": [
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU6QsppQKxga"
      },
      "source": [
        "# Creating dataframe for all senators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hru9LSaOyotp"
      },
      "source": [
        "# Get list of all 100 senators\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "url = 'https://www.socialseer.com/resources/us-senator-twitter-accounts/'\n",
        "req = requests.get(url)\n",
        "html = req.text\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "# Get twitter handles\n",
        "body = soup.find('tbody')\n",
        "td = body.find('td')\n",
        "children = body.findChildren(recursive=False)\n",
        "twitter_handles = []\n",
        "\n",
        "# Loop through children\n",
        "for child in children:\n",
        "  if child.a:\n",
        "    twitter_handles.append(child.a.text)\n",
        "\n",
        "# Removing senators who no longer are in senate, or who changed their handles\n",
        "remove = ['SenKamalaHarris', 'SenBennetCO', 'JeffFlake', 'SenJonKyl', 'SenateMajLdr', 'SenDougJones']\n",
        "twitter_handles = [keep for keep in twitter_handles if keep not in remove]\n",
        "twitter_handles.extend(['SenatorBennet', 'AlexPadilla4CA', 'SenatorSinema', 'SenMarkKelly', 'LeaderMcConnell', 'SenTuberville'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwxF2ok1LUnZ"
      },
      "source": [
        "def list_text2(json_data, total_retrieved):\n",
        "\n",
        "  sw_nltk = stopwords.words('english')\n",
        "  string_appended = []\n",
        "  for x in range(total_retrieved):\n",
        "    for i in json_data['data'][x]:\n",
        "      words = [word for word in i['text'].split() if word.lower() not in sw_nltk]\n",
        "      new_text = \" \".join(words)\n",
        "      string_appended.append(new_text)\n",
        "\n",
        "  str_empty = \" \"\n",
        "  new_string = str_empty.join(string_appended)\n",
        "  comma_separated = ''.join(new_string).split()\n",
        "\n",
        "  count_words = Counter(comma_separated)\n",
        "  return count_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WQOPvvLvkzx"
      },
      "source": [
        "# Get user id of each senator's username\n",
        "\n",
        "bearer_token = 'AAAAAAAAAAAAAAAAAAAAAKWfUAEAAAAAVlRCaBC9G2XcfK12sEaYBSFMQFo%3D2Ras8AORAGYkWjjjZpicOkZWBShWS96mk74MFTDi6zYZLzqKAj'\n",
        "headers = {'Authorization':('Bearer '+ bearer_token)}\n",
        "\n",
        "def get_user_data(username):\n",
        "  url = \"https://api.twitter.com/2/users/by/username/\" + username\n",
        "  response = requests.request(\"GET\", url, headers=headers)\n",
        "  json_data_test = json.loads(response.text)\n",
        "  print(json_data_test)\n",
        "  return json_data_test['data']\n",
        "\n",
        "# get_user_data(\"SenShelby\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2__Wo3JgZic"
      },
      "source": [
        "# Get tweets \n",
        "\n",
        "all_words_dictionary = {}\n",
        "total_retrieved_global = 0\n",
        "\n",
        "def get_tweets(username):\n",
        "  global total_retrieved_global\n",
        "  user_data = get_user_data(username)\n",
        "  bearer_token = 'AAAAAAAAAAAAAAAAAAAAAKWfUAEAAAAAVlRCaBC9G2XcfK12sEaYBSFMQFo%3D2Ras8AORAGYkWjjjZpicOkZWBShWS96mk74MFTDi6zYZLzqKAj'\n",
        "  headers = {'Authorization':('Bearer '+ bearer_token)}\n",
        "\n",
        "  n = 5                        \n",
        "  max_results = 100               \n",
        "  total_retrieved = 0               \n",
        "  next_token = \"\"        \n",
        "  json_data = {'data': []}           \n",
        "\n",
        "  # Gathering tweets\n",
        "  while total_retrieved < n:\n",
        "\n",
        "    if next_token == \"\":\n",
        "      url = f\"https://api.twitter.com/2/users/{user_data['id']}/tweets?exclude=retweets,replies&max_results={max_results}\"\n",
        "    else:\n",
        "      url = f\"https://api.twitter.com/2/users/{user_data['id']}/tweets?exclude=retweets,replies&max_results={max_results}&pagination_token={next_token}\"\n",
        "\n",
        "    # Extra paramteres\n",
        "    # url += f\"&tweet.fields=public_metrics,\"\n",
        "    \n",
        "    # Make request to Twitter API enpoint - returns json object\n",
        "    response = requests.request(\"GET\", url, headers=headers)\n",
        "    json_data_test = json.loads(response.text)\n",
        "    json_data['data'].append(json_data_test['data'])\n",
        "\n",
        "    # Retreive next_token in order to loop through more than 100 tweets at a time\n",
        "    try:\n",
        "      next_token = json_data_test['meta']['next_token']\n",
        "    \n",
        "    except:\n",
        "      break\n",
        "    total_retrieved += 1\n",
        "    total_retrieved_global += 1\n",
        "\n",
        "  # Getting unique words and counts of 200 manchin tweets from list_text() function\n",
        "  words_in_tweet_dictionary = list_text2(json_data, total_retrieved)\n",
        "  for word in words_in_tweet_dictionary.items():\n",
        "    if word[0] not in all_words_dictionary:\n",
        "      all_words_dictionary[word[0]] = 0\n",
        "    all_words_dictionary[word[0]] += word[1]\n",
        "  \n",
        "  return user_data['username'], json_data, total_retrieved"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AffOuz61hIA"
      },
      "source": [
        "sw_nltk = stopwords.words('english')\n",
        "\n",
        "def get_data_frame(sorted_word_list, all_tweets, n=60):\n",
        "  feature_set = set(sorted_word_list[:n])\n",
        "  \n",
        "  instance_list = []\n",
        "  for senator_tweets in all_tweets:\n",
        "    for tweet_batch in senator_tweets[1]['data']:\n",
        "      for tweet_data in tweet_batch:\n",
        "        tweet = tweet_data[\"text\"]\n",
        "        word_count_dictionary = {word: 0 for word in feature_set}\n",
        "\n",
        "        words = [word for word in tweet.split() if word.lower() not in sw_nltk] # get rid of unnecesary words\n",
        "        new_text = \" \".join(words)\n",
        "        comma_separated = ''.join(new_text).split() # comma separated and cleaned tweet \n",
        "\n",
        "        for word in comma_separated:\n",
        "          if word in feature_set:\n",
        "            word_count_dictionary[word] += 1\n",
        "        \n",
        "        word_count_dictionary[\"author_twitter_handle\"] = senator_tweets[0] # 'author_twitter_hanldes' is just a string that we expect won't ever appear in the a tweet and so we're using that for the author's name\n",
        "        instance_list.append(word_count_dictionary)\n",
        "\n",
        "\n",
        "  df = pd.DataFrame(columns=(sorted_word_list[:n] + [\"author_twitter_handle\"]), index=list(range(0, len(instance_list))))\n",
        "  for i, instance in enumerate(instance_list):\n",
        "    df.iloc[i] = instance\n",
        "\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16PHEOLwwm_I"
      },
      "source": [
        "# Scrape tweets\n",
        "all_of_the_tweets = [get_tweets(handle) for handle in twitter_handles]\n",
        "\n",
        "word_count_list = list(all_words_dictionary.items())\n",
        "word_count_list.sort(reverse=True, key=(lambda item: item[1]))\n",
        "sorted_word_list = [word[0] for word in word_count_list]\n",
        "\n",
        "df = get_data_frame(sorted_word_list, all_of_the_tweets)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KIjBw2m5lcY0"
      },
      "source": [
        "df.to_csv('word_count_60.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnUDjZOQaakn"
      },
      "source": [
        "# MLP Model - Dataset w/ 30 features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPzQs9wxaakn"
      },
      "source": [
        "import pandas as pd\n",
        "df_wce = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/word_count_encoding.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5zxH9CVaako",
        "outputId": "a8bc2f84-230e-446b-d6de-7a3219425c93"
      },
      "source": [
        "df_wce.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(48637, 32)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "b-yRkn3Oaako",
        "outputId": "d7ffeb92-5bd6-413b-ec2d-88f42b6d3fc8"
      },
      "source": [
        "df_wce.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "pd.set_option('display.max_columns', 15) #set to None if you dont want an elipses \n",
        "df_wce.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>&amp;amp;</th>\n",
              "      <th>help</th>\n",
              "      <th>American</th>\n",
              "      <th>Senate</th>\n",
              "      <th>need</th>\n",
              "      <th>work</th>\n",
              "      <th>get</th>\n",
              "      <th>...</th>\n",
              "      <th>one</th>\n",
              "      <th>continue</th>\n",
              "      <th>President</th>\n",
              "      <th>protect</th>\n",
              "      <th>Act</th>\n",
              "      <th>–</th>\n",
              "      <th>author_twitter_handle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   &amp;  help  American  Senate  need  work  get  ...  one  continue  \\\n",
              "0      0     0         1       0     0     0    0  ...    0         0   \n",
              "1      0     0         0       0     0     0    0  ...    0         0   \n",
              "2      0     0         0       0     0     0    0  ...    0         0   \n",
              "3      0     0         0       0     0     0    0  ...    0         0   \n",
              "4      0     0         0       0     0     0    0  ...    0         0   \n",
              "5      0     0         0       0     0     0    0  ...    0         0   \n",
              "6      1     0         0       0     0     0    0  ...    1         0   \n",
              "7      1     0         0       0     0     0    0  ...    0         0   \n",
              "8      3     0         0       0     0     1    0  ...    0         0   \n",
              "9      0     0         0       0     0     0    0  ...    0         0   \n",
              "\n",
              "   President  protect  Act  –  author_twitter_handle  \n",
              "0          0        1    0  0              SenShelby  \n",
              "1          0        0    0  0              SenShelby  \n",
              "2          0        1    0  0              SenShelby  \n",
              "3          0        0    0  0              SenShelby  \n",
              "4          0        0    0  0              SenShelby  \n",
              "5          1        0    0  0              SenShelby  \n",
              "6          0        0    0  0              SenShelby  \n",
              "7          0        0    0  0              SenShelby  \n",
              "8          0        0    0  0              SenShelby  \n",
              "9          0        0    0  1              SenShelby  \n",
              "\n",
              "[10 rows x 31 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyCIqw_-aako"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df_wce.drop(columns=['author_twitter_handle'])\n",
        "y = df_wce.author_twitter_handle\n",
        "# y = pd.get_dummies(y)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqvTC9nfx0FQ"
      },
      "source": [
        "## Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "QyVcPg7CnlHN",
        "outputId": "3680a3c0-1b52-480d-e5b4-eecea3c6670f"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Data/word_count_encoding.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>&amp;amp;</th>\n",
              "      <th>help</th>\n",
              "      <th>American</th>\n",
              "      <th>Senate</th>\n",
              "      <th>need</th>\n",
              "      <th>work</th>\n",
              "      <th>get</th>\n",
              "      <th>must</th>\n",
              "      <th>I’m</th>\n",
              "      <th>bill</th>\n",
              "      <th>support</th>\n",
              "      <th>make</th>\n",
              "      <th>people</th>\n",
              "      <th>Biden</th>\n",
              "      <th>health</th>\n",
              "      <th>Americans</th>\n",
              "      <th>time</th>\n",
              "      <th>U.S.</th>\n",
              "      <th>would</th>\n",
              "      <th>bipartisan</th>\n",
              "      <th>working</th>\n",
              "      <th>great</th>\n",
              "      <th>new</th>\n",
              "      <th>across</th>\n",
              "      <th>one</th>\n",
              "      <th>continue</th>\n",
              "      <th>President</th>\n",
              "      <th>protect</th>\n",
              "      <th>Act</th>\n",
              "      <th>–</th>\n",
              "      <th>author_twitter_handle</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SenShelby</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  &amp;  help  American  ...  protect  Act  –  author_twitter_handle\n",
              "0           0      0     0         1  ...        1    0  0              SenShelby\n",
              "1           1      0     0         0  ...        0    0  0              SenShelby\n",
              "2           2      0     0         0  ...        1    0  0              SenShelby\n",
              "3           3      0     0         0  ...        0    0  0              SenShelby\n",
              "4           4      0     0         0  ...        0    0  0              SenShelby\n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9BkoGnJx1bI",
        "outputId": "620c0923-4866-4d0e-ef69-5368ae093200"
      },
      "source": [
        "# Grid search\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "mlp_gs = MLPClassifier(max_iter=600)\n",
        "\n",
        "parameter_space = {\n",
        "    'hidden_layer_sizes': [(64,10),(64,)],\n",
        "    'activation': ['relu', 'logistic'],\n",
        "    'solver': ['sgd', 'adam'],\n",
        "    'alpha': [0.0001, 0.05],\n",
        "    'learning_rate': ['constant','adaptive'],\n",
        "    'momentum': [0.9, 0.65]\n",
        "}\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "# clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\n",
        "clf = GridSearchCV(mlp_gs, parameter_space, n_jobs=-1, cv=5)\n",
        "clf.fit(X_train, y_train) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eJgSDoXyuNv"
      },
      "source": [
        "print('Best parameters found:\\n', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukj5zzQHyuy5"
      },
      "source": [
        "y_true, y_pred = y_test , clf.predict(X_test)\n",
        "from sklearn.metrics import classification_report\n",
        "print('Results on the test set:')\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jv7zttCbO47"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWfxbAiuaako",
        "outputId": "9e6c36b3-36ed-4b35-bcbe-44159010721a"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "mlp_sk = MLPClassifier(learning_rate='constant', learning_rate_init=0.1, hidden_layer_sizes=(60), momentum=0.65, activation='relu', max_iter=500,\n",
        "                       n_iter_no_change=10, solver='adam').fit(X_train, y_train)\n",
        "\n",
        "accuracy = mlp_sk.score(X_test, y_test)\n",
        "epochs = mlp_sk.n_iter_\n",
        "\n",
        "print(f'Test accuracy: {100*round(accuracy,3)}%')\n",
        "print(f'Number of epochs: {epochs}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 5.800000000000001%\n",
            "Number of epochs: 477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIkJjyPYdXIr"
      },
      "source": [
        "## KNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogmUu3yhhJjC",
        "outputId": "34dd9d1a-c80b-4593-dcc0-54f05793b7da"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X_min_max = pd.DataFrame(MinMaxScaler().fit_transform(X))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_min_max, y, test_size=0.2)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=60, weights='distance')\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "score = metrics.accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: \\t {round(score, 5)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: \t 4.749%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrzXNpxziCKu"
      },
      "source": [
        "## SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0d0DAo9iErr",
        "outputId": "53faab49-483c-42a3-dd19-6a36b102ff8f"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X_min_max, y, test_size=0.2)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "SVC_model = SVC()\n",
        "SVC_model.fit(X_train, y_train)\n",
        "SVC_prediction = SVC_model.predict(X_test)\n",
        "print(metrics.accuracy_score(SVC_prediction, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.059004934210526314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAxKnBwti1l2"
      },
      "source": [
        "## Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtLDEhqwi4-l",
        "outputId": "6eb52933-d904-425f-e2ea-aa7450c05584"
      },
      "source": [
        "from sklearn import tree\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "clf_sk = tree.DecisionTreeClassifier(criterion='entropy', min_samples_leaf=2, min_impurity_decrease=.0001)\n",
        "\n",
        "clf_sk = clf_sk.fit(X_train, y_train)\n",
        "\n",
        "accuracy = clf_sk.score(X_test, y_test)\n",
        "print(f'Accuracy of model: {round(accuracy,6)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of model: 4.4921999999999995%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLbN0CXIklVY",
        "outputId": "a1738f4e-61c4-4489-f759-6f2412c248d6"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=10, criterion='entropy', min_samples_leaf=2, min_impurity_decrease=.0001)\n",
        "clf.fit(X_train, y_train)\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f'Accuracy of model: {round(accuracy,6)*100}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of model: 4.9548000000000005%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV1xT59YNJd1"
      },
      "source": [
        "# Word Count Encoding Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcFJlQx4NQIE"
      },
      "source": [
        "## Pipeline Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tguuMLH7NLpd"
      },
      "source": [
        "# Data Processing Methods\n",
        "\n",
        "def get_data(url, col_drop):\n",
        "  import pandas as pd\n",
        "  pd.set_option('display.max_columns', 20)\n",
        "  df = pd.read_csv(url)\n",
        "  for col in col_drop:\n",
        "    try:\n",
        "      df.drop(columns=col, inplace=True)\n",
        "    except:\n",
        "      pass\n",
        "  return df\n",
        "\n",
        "def drop_missing_columns(df, cutoff=0.55):\n",
        "  import pandas as pd\n",
        "  for col in df:\n",
        "    if (df[col].isna().sum() / len(df)) >= cutoff:\n",
        "      df.drop(columns=[col], inplace=True)\n",
        "  return df\n",
        "\n",
        "\n",
        "def binning_categorical(df, cutoff=0.10):\n",
        "  import pandas as pd\n",
        "\n",
        "  for col in df:\n",
        "    if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "      for group, count in df[col].value_counts().iteritems():\n",
        "        if count/len(df) < cutoff:\n",
        "          df.loc[df[col] == group, col] = 'Other'\n",
        "  return df\n",
        "\n",
        "\n",
        "def bin_time(row):\n",
        "    if row['time_day'] >= 0 and row['time_day'] < 7:\n",
        "        return 'midnight'\n",
        "    if row['time_day'] >= 7 and row['time_day'] < 12:\n",
        "        return 'morning'\n",
        "    if row['time_day'] >= 12 and row['time_day'] < 18:\n",
        "        return 'afternoon'\n",
        "    return 'night'\n",
        "\n",
        "\n",
        "def dummy_encode(df):\n",
        "  import pandas as pd\n",
        "  for col in df:\n",
        "    if col != 'author_name':\n",
        "      if not pd.api.types.is_numeric_dtype(df[col]):\n",
        "        return pd.get_dummies(df, columns=[col], drop_first=True)\n",
        "\n",
        "\n",
        "def fix_time_day(df, column):\n",
        "  df[column] = df.apply(lambda row: bin_time(row), axis=1)\n",
        "  return df\n",
        "\n",
        "\n",
        "def min_max_normalization(X):\n",
        "  # Min max normalization\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  from sklearn import preprocessing\n",
        "\n",
        "  min_max_scaler = preprocessing.MinMaxScaler()\n",
        "  return min_max_scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "def z_score_normalization(X):\n",
        "  # Normailze dataframe\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  from sklearn import preprocessing\n",
        "\n",
        "  scaler = preprocessing.StandardScaler().fit(X)\n",
        "  return scaler.transform(X)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0EdlmE10NS8k"
      },
      "source": [
        "# Algorithms\n",
        "\n",
        "def try_models(X,y):\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn import preprocessing\n",
        "  from sklearn import metrics\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "  fit = {}\n",
        "\n",
        "  print('Trying MLP model')\n",
        "  from sklearn.neural_network import MLPClassifier\n",
        "  # mlp = MLPClassifier(learning_rate='constant', learning_rate_init=0.05, hidden_layer_sizes=(20), momentum=0.9, activation='relu', max_iter=1500,\n",
        "  #                      n_iter_no_change=20, solver='sgd').fit(X_train, y_train)\n",
        "  mlp = MLPClassifier(max_iter=500).fit(X_train, y_train)\n",
        "  fit['MLP'] = [mlp.score(X_test, y_test), mlp]\n",
        "\n",
        "\n",
        "  print('Trying KNN model')\n",
        "  from sklearn.neighbors import KNeighborsClassifier\n",
        "  knn = KNeighborsClassifier(n_neighbors=25, weights='distance')\n",
        "  knn.fit(X_train, y_train)\n",
        "  y_pred = knn.predict(X_test)\n",
        "  fit['KNN'] = [metrics.accuracy_score(y_test, y_pred), knn]\n",
        "\n",
        "\n",
        "  print('Trying SVC model')\n",
        "  from sklearn.svm import SVC\n",
        "  SVC_model = SVC()\n",
        "  SVC_model.fit(X_train, y_train)\n",
        "  SVC_prediction = SVC_model.predict(X_test)\n",
        "  fit['SVC'] = [metrics.accuracy_score(SVC_prediction, y_test), SVC_model]\n",
        "\n",
        "\n",
        "  print('Trying Decision Tree model')\n",
        "  from sklearn import tree\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  DT_clf = tree.DecisionTreeClassifier(criterion='entropy', min_samples_leaf=2, min_impurity_decrease=.0001)  \n",
        "  DT_clf.fit(X_train, y_train)\n",
        "  fit['DecisionTree'] = [DT_clf.score(X_test, y_test), DT_clf]\n",
        "\n",
        "\n",
        "  print('Trying Random Forest model\\n')\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "  RFC = RandomForestClassifier(n_estimators=10)\n",
        "  RFC.fit(X_train, y_train)\n",
        "  fit['RandomForest'] = [RFC.score(X_test, y_test), RFC]\n",
        "\n",
        "  return fit\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK6RJYguNYF1"
      },
      "source": [
        "## Run Algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An0FCvC9Najy",
        "outputId": "e4b73a6f-9b9d-4f2c-f4bf-cde3ce7efc87"
      },
      "source": [
        "# Full test - Normal 30 features\n",
        "\n",
        "df = get_data('/content/drive/MyDrive/Colab Notebooks/Data/word_count_encoding.csv', col_drop=['Unnamed: 0'])\n",
        "\n",
        "X = df.drop(columns=['author_twitter_handle']) # dropping unnecesary columns\n",
        "y = df['author_twitter_handle']\n",
        "\n",
        "\n",
        "# # Deciding to normalize w/ min_max\n",
        "# X = min_max_normalization(X)\n",
        "\n",
        "\n",
        "fit = try_models(X, y)\n",
        "\n",
        "accuracy = sorted(fit, key=fit.__getitem__, reverse=True)\n",
        "for acc in accuracy:\n",
        "  print(f'{acc}:\\t{fit[acc][0]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying MLP model\n",
            "Trying KNN model\n",
            "Trying SVC model\n",
            "Trying Decision Tree model\n",
            "Trying Random Forest model\n",
            "\n",
            "SVC:\t0.06044407894736842\n",
            "MLP:\t0.058182565789473686\n",
            "DecisionTree:\t0.04615542763157895\n",
            "RandomForest:\t0.04584703947368421\n",
            "KNN:\t0.041426809210526314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qN0RKwAiS41x",
        "outputId": "7821ad14-1cdc-45e8-a755-7128173b5628"
      },
      "source": [
        "# Full test - 20 Features\n",
        "\n",
        "df = get_data('/content/drive/MyDrive/Colab Notebooks/Data/word_count_encoding.csv', col_drop=['Unnamed: 0'])\n",
        "\n",
        "X = df.drop(columns=['author_twitter_handle']) # dropping unnecesary columns\n",
        "X = X[X.columns[:-10]]\n",
        "y = df['author_twitter_handle']\n",
        "\n",
        "\n",
        "# # Deciding to normalize w/ min_max\n",
        "# X = min_max_normalization(X)\n",
        "\n",
        "\n",
        "fit = try_models(X, y)\n",
        "\n",
        "accuracy = sorted(fit, key=fit.__getitem__, reverse=True)\n",
        "for acc in accuracy:\n",
        "  print(f'{acc}:\\t{fit[acc][0]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying MLP model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying KNN model\n",
            "Trying SVC model\n",
            "Trying Decision Tree model\n",
            "Trying Random Forest model\n",
            "\n",
            "SVC:\t0.04399671052631579\n",
            "MLP:\t0.04358552631578947\n",
            "DecisionTree:\t0.03834292763157895\n",
            "RandomForest:\t0.038240131578947366\n",
            "KNN:\t0.028885690789473683\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59QEMHwuT6ez",
        "outputId": "71ac5878-e97c-49a7-b097-98d0c9ffbc07"
      },
      "source": [
        "# Full test - 10 Features\n",
        "\n",
        "df = get_data('/content/drive/MyDrive/Colab Notebooks/Data/word_count_encoding.csv', col_drop=['Unnamed: 0'])\n",
        "\n",
        "X = df.drop(columns=['author_twitter_handle']) # dropping unnecesary columns\n",
        "X = X[X.columns[:-20]]\n",
        "y = df['author_twitter_handle']\n",
        "\n",
        "\n",
        "# # Deciding to normalize w/ min_max\n",
        "# X = min_max_normalization(X)\n",
        "\n",
        "\n",
        "fit = try_models(X, y)\n",
        "\n",
        "accuracy = sorted(fit, key=fit.__getitem__, reverse=True)\n",
        "for acc in accuracy:\n",
        "  print(f'{acc}:\\t{fit[acc][0]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying MLP model\n",
            "Trying KNN model\n",
            "Trying SVC model\n",
            "Trying Decision Tree model\n",
            "Trying Random Forest model\n",
            "\n",
            "SVC:\t0.030838815789473683\n",
            "MLP:\t0.03022203947368421\n",
            "RandomForest:\t0.02837171052631579\n",
            "DecisionTree:\t0.02826891447368421\n",
            "KNN:\t0.02045641447368421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx6MGh5QnfTI"
      },
      "source": [
        "More"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQN3rSe4m0GE",
        "outputId": "e7399b52-8fa7-43f1-ef7f-52befc12b4c9"
      },
      "source": [
        "# Full test - Normal 60 features\n",
        "\n",
        "df = get_data('/content/drive/MyDrive/Colab Notebooks/Data/word_count_60.csv', col_drop=['Unnamed: 0'])\n",
        "\n",
        "X = df.drop(columns=['author_twitter_handle']) # dropping unnecesary columns\n",
        "y = df['author_twitter_handle']\n",
        "\n",
        "\n",
        "# # Deciding to normalize w/ min_max\n",
        "# X = min_max_normalization(X)\n",
        "\n",
        "\n",
        "fit = try_models(X, y)\n",
        "\n",
        "accuracy = sorted(fit, key=fit.__getitem__, reverse=True)\n",
        "for acc in accuracy:\n",
        "  print(f'{acc}:\\t{fit[acc][0]}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying MLP model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying KNN model\n",
            "Trying SVC model\n",
            "Trying Decision Tree model\n",
            "Trying Random Forest model\n",
            "\n",
            "SVC:\t0.08145402121305735\n",
            "MLP:\t0.06961178045515395\n",
            "RandomForest:\t0.05478323550612707\n",
            "KNN:\t0.05447430748635568\n",
            "DecisionTree:\t0.05272371537431778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qygWrqYvnTY6",
        "outputId": "4861ad33-fd57-41aa-8404-14190de9a94b"
      },
      "source": [
        "# Full test - Normal 50 features\n",
        "\n",
        "df = get_data('/content/drive/MyDrive/Colab Notebooks/Data/word_count_60.csv', col_drop=['Unnamed: 0'])\n",
        "\n",
        "X = df.drop(columns=['author_twitter_handle']) # dropping label\n",
        "X = X[X.columns[:-10]]\n",
        "y = df['author_twitter_handle']\n",
        "\n",
        "\n",
        "fit = try_models(X, y)\n",
        "\n",
        "accuracy = sorted(fit, key=fit.__getitem__, reverse=True)\n",
        "for acc in accuracy:\n",
        "  print(f'{acc}:\\t{fit[acc][0]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying MLP model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trying KNN model\n",
            "Trying SVC model\n",
            "Trying Decision Tree model\n",
            "Trying Random Forest model\n",
            "\n",
            "SVC:\t0.07403974873854392\n",
            "MLP:\t0.0668314282772114\n",
            "RandomForest:\t0.051796931315003604\n",
            "DecisionTree:\t0.0474719390382041\n",
            "KNN:\t0.047265987025023166\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKjx9aaunYE7",
        "outputId": "69fb00db-286e-4e8d-d5b6-7a9fe8bbbc2a"
      },
      "source": [
        "# Full test - Normal 40 features\n",
        "\n",
        "df = get_data('/content/drive/MyDrive/Colab Notebooks/Data/word_count_60.csv', col_drop=['Unnamed: 0'])\n",
        "\n",
        "X = df.drop(columns=['author_twitter_handle']) # dropping label\n",
        "X = X[X.columns[:-20]]\n",
        "y = df['author_twitter_handle']\n",
        "\n",
        "\n",
        "fit = try_models(X, y)\n",
        "\n",
        "accuracy = sorted(fit, key=fit.__getitem__, reverse=True)\n",
        "for acc in accuracy:\n",
        "  print(f'{acc}:\\t{fit[acc][0]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying MLP model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying KNN model\n",
            "Trying SVC model\n",
            "Trying Decision Tree model\n",
            "Trying Random Forest model\n",
            "\n",
            "SVC:\t0.07342189269900114\n",
            "MLP:\t0.0639481000926784\n",
            "RandomForest:\t0.05231181134795593\n",
            "DecisionTree:\t0.05025229121614664\n",
            "KNN:\t0.04819277108433735\n"
          ]
        }
      ]
    }
  ]
}